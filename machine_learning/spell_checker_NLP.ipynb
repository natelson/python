{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spell_checker_NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3i8SeuKkgAoHSbGUtlsET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natelson/python/blob/main/machine_learning/spell_checker_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo of Spell Checker with NLP (Natural Language Processing)"
      ],
      "metadata": {
        "id": "knpnXfZGUxLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The idea here is show some functions the Library of Python [nltk](https://www.nltk.org/api/nltk.html). For this example, I created a litle demo of spell checker. The main idea is to go through the following steps:\n",
        "\n",
        "> * Read a database with a lot of words, this database is a source of training to \n",
        "learn words.\n",
        "* Normalize these words.\n",
        "* Create the processing of tokenization of these words.\n",
        "* Create functions that analyze the best answer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81xvikahVQoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "ozSdHiMVJQLJ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "NczY2xjiIB0i"
      },
      "outputs": [],
      "source": [
        "def get_text_from_a_url_file(target_url :str):\n",
        "  response = requests.get(target_url)\n",
        "  data = response.text\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read our database\n",
        "\n",
        "> When we work in NLP, our database is known as a corpus. That is, it is a body composed of several texts, and each one corresponds to an article on our blog, called a document.\n",
        "\n",
        "> Therefore, the corpus is a set of documents in Natural Language Processing, and in our case, it is composed of blog articles that form the articles.txt database. Then we will read it in the notebook.\n"
      ],
      "metadata": {
        "id": "_5EHgonh_f6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_url = 'https://raw.githubusercontent.com/natelson/python/main/machine_learning/data/articles.txt'\n",
        "articles = get_text_from_a_url_file(target_url)"
      ],
      "metadata": {
        "id": "97c4yZEc1JBC"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Size of Vocabulary\n",
        "\n",
        "> Let's imagine that two students are learning a new language, Portuguese.\n",
        "\n",
        "> At the end of a year of studies, the first read only a 100-page book, while the other read the entire Game of Thrones collection in Portuguese, with each part having almost a thousand pages. In this case, the second person has much more information about the language, as they have access to a much larger vocabulary, including being able to make corrections.\n",
        "\n",
        "> Therefore, the number of words is interesting for this learning, which also applies to our spell checker.\n",
        "\n",
        "> To know if our vocabulary is ideal for the job, we must know how many words it has. After all, a text with ten words makes at most ten corrections, for example. So we will need to have a corpus with a large volume of terms.\n",
        "\n",
        "> In this case, using only the len() function receiving articles will not help us, as it will only return the number of characters present in the vocabulary, and not the number of words we want.\n"
      ],
      "metadata": {
        "id": "ktqZ3eS0EK3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(articles)"
      ],
      "metadata": {
        "id": "Pq0ppC9oGZOi",
        "outputId": "88b6987c-d71e-41f6-a8d0-86a063f0953b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2605046"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus\n",
        "\n",
        "> A corpus is a collection of authentic text or audio organized into datasets. Authentic here means text written or audio spoken by a native of the language or dialect. A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets\n",
        "\n",
        "> In our case, our corpus comes from articles in Portuguese that are in our articles.txt database\n"
      ],
      "metadata": {
        "id": "3HHVYFGVGx6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## String Tokenization\n",
        "\n",
        "> String tokenization is a process where a string is broken into several parts. Each part is called a token. For example, if “I am going” is a string, the discrete parts—such as “I”, “am”, and “going”—are the tokens.\n",
        "\n",
        "> The process of transforming a text file into small tokens is called Tokenization, which is recurrent in the pre-processing or statistical analysis of textual data. So, as it is widely used, we have tools that facilitate its implementation.\n",
        "\n",
        "> A well-known one in the field of NLP is the nltk https://www.nltk.org/ or Natural Language Tool Kit, which is a set of tools that implements several methods and algorithms for textual analysis.\n",
        "\n",
        "> Let's import the nltk package and use the tokenize class, which has tokenization methods implemented, such as word_tokenize() which we will call next. We will also need the punkt package as shown below."
      ],
      "metadata": {
        "id": "LymEzWhY_91w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxK1H-Flv2bi",
        "outputId": "4c2da576-dce3-491b-a558-4d26cd80593d"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a token of our database\n",
        "\n",
        "> For this example, we are going to use the word_tokenize function which teturn a tokenized copy of text, using NLTK’s recommended word tokenizer. https://www.geeksforgeeks.org/python-nltk-nltk-tokenizer-word_tokenize/"
      ],
      "metadata": {
        "id": "AqsdB_e4JbQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.tokenize.word_tokenize(articles, language='portuguese')\n",
        "print(f'Size of tokens before only_words : {len(tokens)}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OaFxOEyOLxIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ef918a-648d-411f-afd5-e419733eddd1"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of tokens before only_words : 490809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Our token has a size of 490809, however it has other items that are not part of my vocabulary, punctuation for example. So we need to filter our token so that it only has alphanumeric words. And we implement this in the function below choose_only_words"
      ],
      "metadata": {
        "id": "3LoVUFzANX9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_only_words(token_list):\n",
        "  token_words = []\n",
        "  for token in token_list:\n",
        "    if token.isalpha():\n",
        "      token_words.append(token)\n",
        "\n",
        "  return token_words"
      ],
      "metadata": {
        "id": "sLIsdYj7r23X"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_only_words = choose_only_words(tokens)\n",
        "print(f'Size of tokens after only_words : {len(tokens_only_words)}')"
      ],
      "metadata": {
        "id": "Bsu5VbDFLfp0",
        "outputId": "7ed95d07-c2e4-4b11-f012-4c97cc9e50e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of tokens after only_words : 393935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How many words are in my corpus list?"
      ],
      "metadata": {
        "id": "lVFGZ-8C3d0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The total value of 393935 words previously returned in our corpus does not represent the amount of unique words that can be corrected, after all many of them will appear *repeated *in the texts.\n",
        "\n",
        "> Therefore, we will need to calculate how many unique words there are without repetition.\n",
        "\n",
        "> Therefore, we will have to normalize the corpus and turn all the text into lowercase.\n",
        "\n",
        "> We will define the normalize_token() function with def, then we will pass the word_list and return the normalized_list, which will only be composed of words whose characters have all been converted to lowercase. This will be done with word.lower() being the parameter of .append() of the normalized_list.\n"
      ],
      "metadata": {
        "id": "L0F4agvF-zuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_token(token_list : list):\n",
        "  for index in range(len(token_list)):\n",
        "    token_list[index] = str(token_list[index]).lower()\n",
        "  \n",
        "  return token_list\n"
      ],
      "metadata": {
        "id": "wzU2bWRg1Uk4"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_only_words = normalize_token(tokens_only_words)"
      ],
      "metadata": {
        "id": "1vTv2AJi3q7e"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_only_words[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhwAkAh__997",
        "outputId": "58d91b2f-6ab8-485d-ca45-f27b24aa86d0"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['imagem', 'temos', 'a', 'seguinte', 'classe', 'que', 'representa', 'um', 'usuário', 'no', 'nosso', 'sistema', 'java', 'para', 'salvar', 'um', 'novo', 'usuário', 'várias', 'validações', 'são', 'feitas', 'como', 'por', 'exemplo', 'ver', 'se', 'o', 'nome', 'só', 'contém', 'letras', 'cpf', 'só', 'e', 'ver', 'se', 'o', 'usuário', 'possui', 'no', 'mínimo', 'anos', 'veja', 'o', 'método', 'que', 'faz', 'essa', 'validação']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> Now that we have our normalized list, we can remove word repetitions, and there are a few ways to do that.\n",
        "\n",
        "> We can either build an algorithm or work with more interesting ideas in Python, as it has methods that implement the idea of mathematical sets, such as set().\n",
        "\n",
        "> With that, we pass our normalized list to set() in the next cell, removing the repetitions.\n",
        "\n"
      ],
      "metadata": {
        "id": "msK_x6nqCOWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_only_words_unique = set(tokens_only_words)"
      ],
      "metadata": {
        "id": "5toLT_1WBkyy"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Size of tokens after normalize and remove duplicates : {len(tokens_only_words_unique)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UymttWVWCr1r",
        "outputId": "635c2f11-9ae3-4752-add3-715596571c9a"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of tokens after normalize and remove duplicates : 17653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Running the code, we'll get 17653 of different types, which is quite an interesting value.\n",
        "\n",
        "> This will be the number of words that our spell checker will learn to correct, remembering that we have a total of 393,935 words in the corpus.\n",
        "\n",
        "> According to t[his link with a Duolingo article](https://www.optilingo.com/blog/general/how-many-words-do-you-need-to-know-to-become-fluent-in-a-language/), you need to know about 800 to 1,000 words of a language to be at a basic level, 1,500 to 2,000 for intermediate, and 3,000 to 4,000 words to be fluent.\n",
        "\n",
        "> Therefore, as our algorithm will learn to correct more than 17,000 questions, it is more than enough for us to build a good spell checker, without taking into account the variations between words and other details that we will see later."
      ],
      "metadata": {
        "id": "YDIqBUEUN-VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if I spelled it wrong?\n",
        "> We want our spell checker to get the term \"logic\" misspelled and return it corrected as \"logic\".\n",
        "\n",
        "> We will discover the correct word because it is among the more than 17,000 words known by counting previously.\n",
        "\n",
        "> However, we have not yet implemented the transition from \"lgica\" to \"lógica\" in fact.\n",
        "\n",
        "> We will take the misspelled word and generate new words, where one of them could be the correct one. In our specific problem, we have the letter \"ó\" less.\n",
        "\n",
        "> Our NLP model will need an algorithm capable of inserting an extra letter into this misspelled term. The algorithm below solves this.\n",
        "\n",
        ">For this, we need to take the typed word and generate numerous variations of it to see which one is correct, so we can insert a missing letter, invert a letter or even remove a letter.\n",
        "\n",
        "> These implementations follow below"
      ],
      "metadata": {
        "id": "PxUZAV2UXHIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Take a list of one-word tuples and insert a letter into each of the list and return, example of passed list [('','lgica'), ('l','gica'), ('lg','ica'), ..] and returned list [('a','lgica'), ('la','gica'), ('lga','ica'), ..]."
      ],
      "metadata": {
        "id": "7jMRnaNIV_YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_letter_with_tuple_list(list_tuples_word :list):\n",
        "  new_words = []\n",
        "  letters = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
        "  for right, left in list_tuples_word:\n",
        "    for letter in letters:\n",
        "      new_words.append(right + letter + left)\n",
        "\n",
        "  return new_words\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6m0vi4NqIgqu"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Take a list of one-word tuples and swap a letter into each of the list and return, example of passed list [('','lgica'), ('l','gica'), ('lg','ica'), ..] and returned list [('g','lica'), ('li','gca'), ('lgc','ia'), ..]."
      ],
      "metadata": {
        "id": "pRuK-4eQWgYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_letter_with_tuple_list(list_tuples_word :list):\n",
        "  new_words = []\n",
        "  letters = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
        "  for right, left in list_tuples_word:\n",
        "    for letter in letters:\n",
        "      new_words.append(right + letter + left[1:])\n",
        "\n",
        "  return new_words"
      ],
      "metadata": {
        "id": "0x-FaPXAE8Bz"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Take a list of one-word tuples and remove a letter into each of the list and return, example of passed list [('','lgica'), ('l','gica'), ('lg','ica'), ..] and returned list [('','gica'), ('l','ica'), ('lg','ca'), ..]."
      ],
      "metadata": {
        "id": "iH5sglCAW-kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_letter_with_tuple_list(list_tuples_word :list):\n",
        "  new_words = []\n",
        "  for right, left in list_tuples_word:\n",
        "      new_words.append(right + left[1:])\n",
        "\n",
        "  return new_words"
      ],
      "metadata": {
        "id": "fRGUsvZ17QDo"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_letter_with_tuple_list(list_tuples_word :list):\n",
        "  new_words = []\n",
        "  for right, left in list_tuples_word:\n",
        "      if len(left) > 1:\n",
        "        new_words.append(right + left[1]+ left[0] + left[2:])\n",
        "\n",
        "  return new_words"
      ],
      "metadata": {
        "id": "f2EFUHRkJ_hu"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_variations(word_base :str):\n",
        "  list_tuples_word = []\n",
        "  for i in range(len(word_base) +1):\n",
        "    list_tuples_word.append((word_base[:i], word_base[i:]))\n",
        "  \n",
        "  word_variotions_list =  concatenate_letter_with_tuple_list(list_tuples_word)\n",
        "  word_variotions_list +=  remove_letter_with_tuple_list(list_tuples_word)\n",
        "  word_variotions_list +=  change_letter_with_tuple_list(list_tuples_word)\n",
        "  word_variotions_list +=  reverse_letter_with_tuple_list(list_tuples_word)\n",
        "  return word_variotions_list\n"
      ],
      "metadata": {
        "id": "tt_bY1B1C2cj"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_variations_list = word_variations('ólgica')\n",
        "print(word_variations_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utM2oUdBIeWg",
        "outputId": "aaf30b16-8638-4680-9f32-c4026d835949"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aólgica', 'bólgica', 'cólgica', 'dólgica', 'eólgica', 'fólgica', 'gólgica', 'hólgica', 'iólgica', 'jólgica', 'kólgica', 'lólgica', 'mólgica', 'nólgica', 'oólgica', 'pólgica', 'qólgica', 'rólgica', 'sólgica', 'tólgica', 'uólgica', 'vólgica', 'wólgica', 'xólgica', 'yólgica', 'zólgica', 'àólgica', 'áólgica', 'âólgica', 'ãólgica', 'èólgica', 'éólgica', 'êólgica', 'ìólgica', 'íólgica', 'îólgica', 'òólgica', 'óólgica', 'ôólgica', 'õólgica', 'ùólgica', 'úólgica', 'ûólgica', 'çólgica', 'óalgica', 'óblgica', 'óclgica', 'ódlgica', 'óelgica', 'óflgica', 'óglgica', 'óhlgica', 'óilgica', 'ójlgica', 'óklgica', 'óllgica', 'ómlgica', 'ónlgica', 'óolgica', 'óplgica', 'óqlgica', 'órlgica', 'óslgica', 'ótlgica', 'óulgica', 'óvlgica', 'ówlgica', 'óxlgica', 'óylgica', 'ózlgica', 'óàlgica', 'óálgica', 'óâlgica', 'óãlgica', 'óèlgica', 'óélgica', 'óêlgica', 'óìlgica', 'óílgica', 'óîlgica', 'óòlgica', 'óólgica', 'óôlgica', 'óõlgica', 'óùlgica', 'óúlgica', 'óûlgica', 'óçlgica', 'ólagica', 'ólbgica', 'ólcgica', 'óldgica', 'ólegica', 'ólfgica', 'ólggica', 'ólhgica', 'óligica', 'óljgica', 'ólkgica', 'óllgica', 'ólmgica', 'ólngica', 'ólogica', 'ólpgica', 'ólqgica', 'ólrgica', 'ólsgica', 'óltgica', 'ólugica', 'ólvgica', 'ólwgica', 'ólxgica', 'ólygica', 'ólzgica', 'ólàgica', 'ólágica', 'ólâgica', 'ólãgica', 'ólègica', 'ólégica', 'ólêgica', 'ólìgica', 'ólígica', 'ólîgica', 'ólògica', 'ólógica', 'ólôgica', 'ólõgica', 'ólùgica', 'ólúgica', 'ólûgica', 'ólçgica', 'ólgaica', 'ólgbica', 'ólgcica', 'ólgdica', 'ólgeica', 'ólgfica', 'ólggica', 'ólghica', 'ólgiica', 'ólgjica', 'ólgkica', 'ólglica', 'ólgmica', 'ólgnica', 'ólgoica', 'ólgpica', 'ólgqica', 'ólgrica', 'ólgsica', 'ólgtica', 'ólguica', 'ólgvica', 'ólgwica', 'ólgxica', 'ólgyica', 'ólgzica', 'ólgàica', 'ólgáica', 'ólgâica', 'ólgãica', 'ólgèica', 'ólgéica', 'ólgêica', 'ólgìica', 'ólgíica', 'ólgîica', 'ólgòica', 'ólgóica', 'ólgôica', 'ólgõica', 'ólgùica', 'ólgúica', 'ólgûica', 'ólgçica', 'ólgiaca', 'ólgibca', 'ólgicca', 'ólgidca', 'ólgieca', 'ólgifca', 'ólgigca', 'ólgihca', 'ólgiica', 'ólgijca', 'ólgikca', 'ólgilca', 'ólgimca', 'ólginca', 'ólgioca', 'ólgipca', 'ólgiqca', 'ólgirca', 'ólgisca', 'ólgitca', 'ólgiuca', 'ólgivca', 'ólgiwca', 'ólgixca', 'ólgiyca', 'ólgizca', 'ólgiàca', 'ólgiáca', 'ólgiâca', 'ólgiãca', 'ólgièca', 'ólgiéca', 'ólgiêca', 'ólgiìca', 'ólgiíca', 'ólgiîca', 'ólgiòca', 'ólgióca', 'ólgiôca', 'ólgiõca', 'ólgiùca', 'ólgiúca', 'ólgiûca', 'ólgiçca', 'ólgicaa', 'ólgicba', 'ólgicca', 'ólgicda', 'ólgicea', 'ólgicfa', 'ólgicga', 'ólgicha', 'ólgicia', 'ólgicja', 'ólgicka', 'ólgicla', 'ólgicma', 'ólgicna', 'ólgicoa', 'ólgicpa', 'ólgicqa', 'ólgicra', 'ólgicsa', 'ólgicta', 'ólgicua', 'ólgicva', 'ólgicwa', 'ólgicxa', 'ólgicya', 'ólgicza', 'ólgicàa', 'ólgicáa', 'ólgicâa', 'ólgicãa', 'ólgicèa', 'ólgicéa', 'ólgicêa', 'ólgicìa', 'ólgicía', 'ólgicîa', 'ólgicòa', 'ólgicóa', 'ólgicôa', 'ólgicõa', 'ólgicùa', 'ólgicúa', 'ólgicûa', 'ólgicça', 'ólgicaa', 'ólgicab', 'ólgicac', 'ólgicad', 'ólgicae', 'ólgicaf', 'ólgicag', 'ólgicah', 'ólgicai', 'ólgicaj', 'ólgicak', 'ólgical', 'ólgicam', 'ólgican', 'ólgicao', 'ólgicap', 'ólgicaq', 'ólgicar', 'ólgicas', 'ólgicat', 'ólgicau', 'ólgicav', 'ólgicaw', 'ólgicax', 'ólgicay', 'ólgicaz', 'ólgicaà', 'ólgicaá', 'ólgicaâ', 'ólgicaã', 'ólgicaè', 'ólgicaé', 'ólgicaê', 'ólgicaì', 'ólgicaí', 'ólgicaî', 'ólgicaò', 'ólgicaó', 'ólgicaô', 'ólgicaõ', 'ólgicaù', 'ólgicaú', 'ólgicaû', 'ólgicaç', 'lgica', 'ógica', 'ólica', 'ólgca', 'ólgia', 'ólgic', 'ólgica', 'algica', 'blgica', 'clgica', 'dlgica', 'elgica', 'flgica', 'glgica', 'hlgica', 'ilgica', 'jlgica', 'klgica', 'llgica', 'mlgica', 'nlgica', 'olgica', 'plgica', 'qlgica', 'rlgica', 'slgica', 'tlgica', 'ulgica', 'vlgica', 'wlgica', 'xlgica', 'ylgica', 'zlgica', 'àlgica', 'álgica', 'âlgica', 'ãlgica', 'èlgica', 'élgica', 'êlgica', 'ìlgica', 'ílgica', 'îlgica', 'òlgica', 'ólgica', 'ôlgica', 'õlgica', 'ùlgica', 'úlgica', 'ûlgica', 'çlgica', 'óagica', 'óbgica', 'ócgica', 'ódgica', 'óegica', 'ófgica', 'óggica', 'óhgica', 'óigica', 'ójgica', 'ókgica', 'ólgica', 'ómgica', 'óngica', 'óogica', 'ópgica', 'óqgica', 'órgica', 'ósgica', 'ótgica', 'óugica', 'óvgica', 'ówgica', 'óxgica', 'óygica', 'ózgica', 'óàgica', 'óágica', 'óâgica', 'óãgica', 'óègica', 'óégica', 'óêgica', 'óìgica', 'óígica', 'óîgica', 'óògica', 'óógica', 'óôgica', 'óõgica', 'óùgica', 'óúgica', 'óûgica', 'óçgica', 'ólaica', 'ólbica', 'ólcica', 'óldica', 'óleica', 'ólfica', 'ólgica', 'ólhica', 'óliica', 'óljica', 'ólkica', 'óllica', 'ólmica', 'ólnica', 'óloica', 'ólpica', 'ólqica', 'ólrica', 'ólsica', 'óltica', 'óluica', 'ólvica', 'ólwica', 'ólxica', 'ólyica', 'ólzica', 'ólàica', 'óláica', 'ólâica', 'ólãica', 'ólèica', 'óléica', 'ólêica', 'ólìica', 'ólíica', 'ólîica', 'ólòica', 'ólóica', 'ólôica', 'ólõica', 'ólùica', 'ólúica', 'ólûica', 'ólçica', 'ólgaca', 'ólgbca', 'ólgcca', 'ólgdca', 'ólgeca', 'ólgfca', 'ólggca', 'ólghca', 'ólgica', 'ólgjca', 'ólgkca', 'ólglca', 'ólgmca', 'ólgnca', 'ólgoca', 'ólgpca', 'ólgqca', 'ólgrca', 'ólgsca', 'ólgtca', 'ólguca', 'ólgvca', 'ólgwca', 'ólgxca', 'ólgyca', 'ólgzca', 'ólgàca', 'ólgáca', 'ólgâca', 'ólgãca', 'ólgèca', 'ólgéca', 'ólgêca', 'ólgìca', 'ólgíca', 'ólgîca', 'ólgòca', 'ólgóca', 'ólgôca', 'ólgõca', 'ólgùca', 'ólgúca', 'ólgûca', 'ólgçca', 'ólgiaa', 'ólgiba', 'ólgica', 'ólgida', 'ólgiea', 'ólgifa', 'ólgiga', 'ólgiha', 'ólgiia', 'ólgija', 'ólgika', 'ólgila', 'ólgima', 'ólgina', 'ólgioa', 'ólgipa', 'ólgiqa', 'ólgira', 'ólgisa', 'ólgita', 'ólgiua', 'ólgiva', 'ólgiwa', 'ólgixa', 'ólgiya', 'ólgiza', 'ólgiàa', 'ólgiáa', 'ólgiâa', 'ólgiãa', 'ólgièa', 'ólgiéa', 'ólgiêa', 'ólgiìa', 'ólgiía', 'ólgiîa', 'ólgiòa', 'ólgióa', 'ólgiôa', 'ólgiõa', 'ólgiùa', 'ólgiúa', 'ólgiûa', 'ólgiça', 'ólgica', 'ólgicb', 'ólgicc', 'ólgicd', 'ólgice', 'ólgicf', 'ólgicg', 'ólgich', 'ólgici', 'ólgicj', 'ólgick', 'ólgicl', 'ólgicm', 'ólgicn', 'ólgico', 'ólgicp', 'ólgicq', 'ólgicr', 'ólgics', 'ólgict', 'ólgicu', 'ólgicv', 'ólgicw', 'ólgicx', 'ólgicy', 'ólgicz', 'ólgicà', 'ólgicá', 'ólgicâ', 'ólgicã', 'ólgicè', 'ólgicé', 'ólgicê', 'ólgicì', 'ólgicí', 'ólgicî', 'ólgicò', 'ólgicó', 'ólgicô', 'ólgicõ', 'ólgicù', 'ólgicú', 'ólgicû', 'ólgicç', 'ólgicaa', 'ólgicab', 'ólgicac', 'ólgicad', 'ólgicae', 'ólgicaf', 'ólgicag', 'ólgicah', 'ólgicai', 'ólgicaj', 'ólgicak', 'ólgical', 'ólgicam', 'ólgican', 'ólgicao', 'ólgicap', 'ólgicaq', 'ólgicar', 'ólgicas', 'ólgicat', 'ólgicau', 'ólgicav', 'ólgicaw', 'ólgicax', 'ólgicay', 'ólgicaz', 'ólgicaà', 'ólgicaá', 'ólgicaâ', 'ólgicaã', 'ólgicaè', 'ólgicaé', 'ólgicaê', 'ólgicaì', 'ólgicaí', 'ólgicaî', 'ólgicaò', 'ólgicaó', 'ólgicaô', 'ólgicaõ', 'ólgicaù', 'ólgicaú', 'ólgicaû', 'ólgicaç', 'lógica', 'óglica', 'óligca', 'ólgcia', 'ólgiac']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Did you mean ?\n",
        "> Now that we can take the word typed, generating several variations of it to handle cases of typos, how can we look at our corpus and know which word is most likely to be correct?"
      ],
      "metadata": {
        "id": "QE9fLS2YY-SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In this step, we will calculate the probability of the generated results.\n",
        "\n",
        "> To build the word_max_probability() function in a new cell, we will first need to do this calculation with a word; the frequency of the term “logic” will be the number of times it appears in the corpus in relation to the total number of words. To calculate, nltk has the .FreqDist() function.\n",
        "\n",
        "> For this, we will create a function called list_frequency_words being equal to the method that calculates the Frequency Distribution of words, and we will pass the normalized_list as a parameter"
      ],
      "metadata": {
        "id": "Yd-Bj7sPaLYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_frequency_words(word_list):\n",
        "  return nltk.FreqDist(word_list)"
      ],
      "metadata": {
        "id": "3JiY39loHSzt"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_frequency = list_frequency_words(tokens_only_words)\n",
        "print(words_frequency[\"lógica\"])\n",
        "total_words = len(tokens_only_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtGqYj7HJerd",
        "outputId": "2b925984-66d0-4815-e4f8-19f06133d1df"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In the word_max_probability() function, we will calculate the probability of a word appearing in the corpus."
      ],
      "metadata": {
        "id": "DCyQ09nNahEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_max_probability(word :str):\n",
        "  word_frequency = words_frequency[word]/total_words\n",
        "  return word_frequency\n",
        "\n",
        "print(round(word_max_probability('lógica'),5))"
      ],
      "metadata": {
        "id": "HPcEbk2GHSOW",
        "outputId": "2bf8311d-cbc7-49ac-fcf4-311bac8f4c2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally the spelling correct\n",
        " > In the spell_checker function, we pass a right or wrong word and return the most likely word that the user meant. Then the function generates the list of possible variations of the word and with the word_max_probability function checks which one is more likely to be correct."
      ],
      "metadata": {
        "id": "ArOzYRkLbald"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_checker(word :str):\n",
        "  word_variations_list = word_variations(word)\n",
        "  word_correct = max(word_variations_list, key=word_max_probability)\n",
        "  return word_correct\n"
      ],
      "metadata": {
        "id": "YCRP5nr_Kuzq"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Você quis dizer {spell_checker(\"llógica\")} ?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIMPuJ9aPfOv",
        "outputId": "5a1f7061-e001-4f6b-b059-9c82b1ea5a92"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Você quis dizer lógica ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Você quis dizer {spell_checker(\"mgica\")} ?')"
      ],
      "metadata": {
        "id": "YFYBHTWKcO3k",
        "outputId": "c1c60d67-6113-442e-a202-903f2011b763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Você quis dizer mágica ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do I analyze whether the spell checker is acceptable?\n",
        "\n",
        "> One way to do this is to run the spell checker in a dictionary where you have the wrong word and the correct one, and check if the spell checker, upon receiving the wrong one, can suggest the correct one.\n",
        "\n",
        "> Take the total value of this list and check how many the spell checker got right to measure its effectiveness."
      ],
      "metadata": {
        "id": "nk6iavaDcgT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the database for test"
      ],
      "metadata": {
        "id": "8WHKss6tdyrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_base():\n",
        "  list_words_test = []\n",
        "  target_url = 'https://raw.githubusercontent.com/natelson/python/main/machine_learning/data/words_for_test.txt'\n",
        "  f = get_text_from_a_url_file(target_url).split('\\n')\n",
        "  for line in f:\n",
        "    if len(line) > 2:\n",
        "      word_right, word_wrong = line.split()\n",
        "      list_words_test.append((word_right, word_wrong))\n",
        "\n",
        "  return list_words_test"
      ],
      "metadata": {
        "id": "Xd8fFwZiPrvu"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_test_base())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEoy2aw5e35Q",
        "outputId": "83911ea8-83e5-4396-9d8c-d3b111c98dd3"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('podemos', 'pyodemos'), ('esse', 'esje'), ('já', 'jrá'), ('nosso', 'nossov'), ('são', 'sãêo'), ('dos', 'dosa'), ('muito', 'muifo'), ('imagem', 'iômagem'), ('sua', 'ósua'), ('também', 'tambéùm'), ('ele', 'eme'), ('fazer', 'èazer'), ('temos', 'temfs'), ('essa', 'eàssa'), ('quando', 'quaôdo'), ('vamos', 'vamvos'), ('sobre', 'hsobre'), ('java', 'sjava'), ('das', 'daõs'), ('agora', 'agorah'), ('está', 'eòtá'), ('cada', 'céda'), ('mesmo', 'zmesmo'), ('nos', 'noâ'), ('forma', 'fobma'), ('seja', 'sejéa'), ('então', 'enêão'), ('criar', 'èriar'), ('código', 'cóeigo'), ('caso', 'casío'), ('exemplo', 'áexemplo'), ('tem', 'tĩem'), ('usuário', 'usuárôio'), ('dados', 'dfados'), ('python', 'pgthon'), ('nossa', 'nossah'), ('além', 'alémè'), ('assim', 'asõim'), ('ter', 'teb'), ('até', 'atĩ'), ('bem', 'âem'), ('design', 'desigen'), ('trabalho', 'trabalàho'), ('foi', 'foo'), ('apenas', 'apenaũ'), ('empresa', 'empresà'), ('valor', 'valíor'), ('será', 'serr'), ('entre', 'entke'), ('método', 'méqodo'), ('precisamos', 'precisamops'), ('ainda', 'ainàa'), ('vai', 'van'), ('conteúdo', 'ûconteúdo'), ('seus', 'çeus'), ('eu', 'eû'), ('todos', 'todtos'), ('tempo', 'temeo'), ('sempre', 'semre'), ('qual', 'quakl'), ('ela', 'elaá'), ('só', 'síó'), ('utilizar', 'utiqizar'), ('projeto', 'prhojeto'), ('site', 'siàe'), ('sem', 'seém'), ('pelo', 'peln'), ('alura', 'aléra'), ('dia', 'tdia'), ('tudo', 'tuúo'), ('podemos', 'kpodemos'), ('esse', 'eẽsse'), ('já', 'jé'), ('nosso', 'nçosso'), ('são', 'sãô'), ('dos', 'odos'), ('muito', 'tuito'), ('imagem', 'imõgem'), ('sua', 'siua'), ('também', 'tamvbém'), ('ele', 'elpe'), ('fazer', 'façzer'), ('temos', 'teos'), ('essa', 'eũsa'), ('quando', 'quaìdo'), ('vamos', 'vjmos'), ('sobre', 'sxobre'), ('java', 'jkva'), ('das', 'dms'), ('agora', 'agtora'), ('está', 'esútá'), ('cada', 'cava'), ('mesmo', 'medmo'), ('nos', 'ános'), ('forma', 'forûa'), ('seja', 'smeja'), ('então', 'enjtão'), ('criar', 'criôar'), ('código', 'cóàigo'), ('caso', 'èaso'), ('exemplo', 'exbemplo'), ('tem', 'túem'), ('usuário', 'usuárin'), ('dados', 'daáos'), ('python', 'pythoçn'), ('nossa', 'nossk'), ('além', 'âlém'), ('assim', 'aóssim'), ('ter', 'tãer'), ('até', 'vté'), ('bem', 'búm'), ('design', 'íesign'), ('trabalho', 'trabèalho'), ('foi', 'kfoi'), ('apenas', 'aapenas'), ('empresa', 'pmpresa'), ('valor', 'valoqr'), ('será', 'sçerá'), ('entre', 'entró'), ('método', 'nétodo'), ('precisamos', 'prefcisamos'), ('ainda', 'sainda'), ('vai', 'uai'), ('conteúdo', 'cĩonteúdo'), ('seus', 'sâus'), ('eu', 'ìeu'), ('todos', 'todás'), ('tempo', 'utempo'), ('sempre', 'sempce'), ('qual', 'fual'), ('ela', 'elal'), ('só', 'skó'), ('utilizar', 'utilĩzar'), ('projeto', 'proójeto'), ('site', 'isite'), ('sem', 'secm'), ('pelo', 'pẽlo'), ('alura', 'aluéa'), ('dia', 'dil'), ('tudo', 'tudy'), ('ela', 'qelay'), ('só', 'sód'), ('utilizar', 'dtilizacr'), ('projeto', 'bprojõto'), ('site', 'ysiteo'), ('sem', 'sõêm'), ('pelo', 'peàli'), ('alura', 'asuraó'), ('dia', 'deiìa'), ('tudo', 'tuĩdoì'), ('ela', 'eúaa'), ('só', 'ró'), ('utilizar', 'utilizẽaçr'), ('projeto', 'prêjetó'), ('site', 'sqiqte'), ('sem', 'sũexm'), ('pelo', 'pçlxo'), ('alura', 'uluraa'), ('dia', 'dĩaz'), ('tudo', 'kzudo'), ('corretor', 'correptor'), ('tática', 'trtica'), ('empoderamento', 'ewpoderamento'), ('linux', 'lifux'), ('cachorro', 'cachoçro'), ('gato', 'îgato'), ('cavalo', 'cakvalo'), ('relógio', 'relógiuo'), ('canela', 'canelac'), ('tênis', 'tênisy'), ('ansiosa', 'anciosa'), ('ansiosa', 'ancciosa'), ('ansiosa', 'ansioa'), ('empoderamento', 'empoderamento'), ('asterisco', 'asterístico'), ('gratuito', 'gratuíto'), ('entretido', 'entertido'), ('ritmo', 'ritimo'), ('idiota', 'indiota'), ('tomara', 'tomare'), ('seja', 'seje'), ('prevalecer', 'provalecer'), ('esteja', 'esteje'), ('mendigo', 'mindigo'), ('cérebro', 'célebro'), ('perturbar', 'pertubar')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This first version receives the list of test words, and calls the spell_checker with the wrong word and compares it with what should be the right one and counts the hits and returns the hit percentage."
      ],
      "metadata": {
        "id": "dS7xVYjXd9Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_checker_correct_percentage(test_base):\n",
        "  correct_answears = 0\n",
        "  for word_right, word_wrong in test_base:\n",
        "    word_test = spell_checker(word_wrong)\n",
        "    if word_test == word_right:\n",
        "      correct_answears += 1\n",
        "  \n",
        "  percentual = (correct_answears/len(test_base)) * 100\n",
        "\n",
        "  return round(percentual,2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rQNSGBZXe7GB"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_base = generate_test_base()\n",
        "vocabulary = list(set(tokens_only_words_unique))"
      ],
      "metadata": {
        "id": "Vnj391jH0HVl"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracity of {spell_checker_correct_percentage(test_base)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFuHt3h21ycV",
        "outputId": "bf696c09-4849-42b5-baa8-209d1a2543ff"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracity of 76.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We have a hit rate of 76.34%, but we need to remember that there are two factors that can influence this percentage, the first is our implementation, that is, words that we were unable to identify its variation in our corpus and the second are words that were not taught to our algorithm, so we need to measure the number of unknown words in our database."
      ],
      "metadata": {
        "id": "UArKZqLCe2tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_checker_correct_percentage(test_base, vocabulary):\n",
        "  correct_answears = 0\n",
        "  unknown_vocabulary = 0\n",
        "  for word_right, word_wrong in test_base:\n",
        "    word_test = spell_checker(word_wrong)\n",
        "    unknown_vocabulary += (word_right not in vocabulary)\n",
        "    if word_test == word_right:\n",
        "      correct_answears += 1\n",
        "    \n",
        "      \n",
        "  \n",
        "  percentual = (correct_answears/len(test_base)) * 100\n",
        "  unknown_percentual =  (unknown_vocabulary/len(test_base)) * 100\n",
        "\n",
        "  return round(percentual,2), round(unknown_percentual,2)\n"
      ],
      "metadata": {
        "id": "UWtjRS442G8V"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentual, unknown_percentual = spell_checker_correct_percentage(test_base, vocabulary)"
      ],
      "metadata": {
        "id": "Bo3Hfo0AVoSr"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracity of {percentual}% and unknow of {unknown_percentual}%' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlspzS64WphS",
        "outputId": "c30dead7-3e6c-4f68-eaf1-8fe3f9a9c2f9"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracity of 76.34% and unknow of 6.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Well from our test base, 6.99% of the correct words were not taught to our broker, one way to solve this is to improve our teaching base"
      ],
      "metadata": {
        "id": "Yq_6lbSTfJOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The goal here is not to build the best spell checker, but to explore some NLP functions that would help us build one. Until next time folks."
      ],
      "metadata": {
        "id": "JmGB6nX4fmTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d_9v1PmXfnfs"
      },
      "execution_count": 153,
      "outputs": []
    }
  ]
}